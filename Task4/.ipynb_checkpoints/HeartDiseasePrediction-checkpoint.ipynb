{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6855b11-8ba8-4ccd-a50e-360ac891ebef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'age', 'sex', 'dataset', 'cp', 'trestbps', 'chol', 'fbs',\n",
      "       'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"heart.csv\")\n",
    "\n",
    "# Get and print column names\n",
    "column_names = df.columns\n",
    "print(column_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af09ed-5b9f-419f-a7a0-c8884a7996c0",
   "metadata": {},
   "source": [
    "## Heart Disease Prediction Model\n",
    "\n",
    "In this project, I developed a heart disease prediction model using **Logistic Regression**. Logistic Regression is a statistical method used for binary classification, which predicts the probability that an instance belongs to a particular category.\n",
    "\n",
    "### Steps Involved:\n",
    "\n",
    "1. **Data Loading**: The dataset was loaded using the Pandas library. The dataset used is `heart.csv`, which contains various features related to heart health.\n",
    "\n",
    "2. **Preprocessing**:\n",
    "   - **Label Encoding**: Categorical variables were converted to numerical format using `LabelEncoder` to facilitate the model training.\n",
    "   - **Splitting the Dataset**: The dataset was divided into features (`X`) and the target variable (`y`). The target variable indicates the presence (1) or absence (0) of heart disease.\n",
    "   - **Train-Test Split**: The dataset was split into training (80%) and testing (20%) sets using `train_test_split`.\n",
    "\n",
    "3. **Handling Missing Values**: The `SimpleImputer` was used to replace missing values in the feature matrix with the mean of the respective columns.\n",
    "\n",
    "4. **Feature Scaling**: Standardization of features was performed using `StandardScaler`, which scales the data to have a mean of 0 and a standard deviation of 1, ensuring that the model is not biased towards features with larger ranges.\n",
    "\n",
    "5. **Model Training**: The Logistic Regression model was trained on the standardized training data.\n",
    "\n",
    "6. **Model Evaluation**:\n",
    "   - Predictions were made on the test set.\n",
    "   - The accuracy of the model was calculated using `accuracy_score`.\n",
    "   - A classification report was generated, which includes precision, recall, and F1-score for each class.\n",
    "\n",
    "### Results\n",
    "\n",
    "- The model achieved an accuracy of approximately 53%.\n",
    "- The classification report provides detailed insights into the performance of the model on the test data.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The Logistic Regression model is a simple method for predicting heart disease but the accuracy suggests that this is not most effective model since it is basically a guessing game now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e001e7-a0cd-49f6-9926-0b16dc685d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5271739130434783\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.91      0.76        75\n",
      "           1       0.45      0.46      0.45        54\n",
      "           2       0.14      0.04      0.06        25\n",
      "           3       0.17      0.12      0.14        26\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.53       184\n",
      "   macro avg       0.28      0.31      0.28       184\n",
      "weighted avg       0.44      0.53      0.47       184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachc\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\sachc\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\sachc\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('heart.csv')\n",
    "\n",
    "# Identify categorical columns and apply label encoding\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_cols:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in the feature matrix\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print accuracy and classification report\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376f66c5-1504-44d6-bc80-a75b0d87cd84",
   "metadata": {},
   "source": [
    "## Heart Disease Prediction Model with Neural Network\n",
    "\n",
    "In this section, I implemented a heart disease prediction model using a **Neural Network** built with TensorFlow and Keras. The model aims to classify individuals based on their health features and predict the presence or absence of heart disease.\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "1. **Input Layer**: Accepts the features from the dataset.\n",
    "2. **Hidden Layers**:\n",
    "   - The first layer consists of 64 neurons with ReLU activation.\n",
    "   - Dropout layers (30% dropout rate) were added after the first two hidden layers to prevent overfitting.\n",
    "   - Additional hidden layers with 32 and 16 neurons, respectively, also use ReLU activation.\n",
    "3. **Output Layer**: Uses softmax activation to predict the class probabilities for the target variable.\n",
    "\n",
    "### Training Process\n",
    "\n",
    "- The model was compiled with the **Adam** optimizer and trained using **categorical crossentropy** as the loss function, suitable for multi-class classification.\n",
    "- The model was trained for 100 epochs with a batch size of 16, and a validation split of 20% was used to monitor performance during training.\n",
    "\n",
    "### Results\n",
    "\n",
    "- The neural network model achieved a **test accuracy of 54%**. This is a slight improvement over the previous Logistic Regression model, which demonstrated lower accuracy.\n",
    "  \n",
    "\n",
    "Test Accuracy: 0.54\n",
    "\n",
    "### Conclusion\n",
    "The neural network model demonstrates a slightly better accuracy than the previous Logistic Regression model. While the improvement is modest, it highlights the potential of using more complex models like neural networks for this classification task. Future work may include hyperparameter tuning, exploring different architectures, and using additional data to enhance model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb63ad41-e23d-440d-bdc7-491a2f0485c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachc\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step - accuracy: 0.2318 - loss: 1.6024 - val_accuracy: 0.5203 - val_loss: 1.3971\n",
      "Epoch 2/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4435 - loss: 1.3792 - val_accuracy: 0.5676 - val_loss: 1.2379\n",
      "Epoch 3/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5035 - loss: 1.2523 - val_accuracy: 0.6081 - val_loss: 1.1053\n",
      "Epoch 4/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5416 - loss: 1.1271 - val_accuracy: 0.6351 - val_loss: 1.0303\n",
      "Epoch 5/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5524 - loss: 1.1185 - val_accuracy: 0.6419 - val_loss: 1.0030\n",
      "Epoch 6/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5573 - loss: 1.0712 - val_accuracy: 0.6554 - val_loss: 0.9710\n",
      "Epoch 7/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5724 - loss: 1.0537 - val_accuracy: 0.6419 - val_loss: 0.9620\n",
      "Epoch 8/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5185 - loss: 1.0945 - val_accuracy: 0.6554 - val_loss: 0.9415\n",
      "Epoch 9/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5813 - loss: 1.0078 - val_accuracy: 0.6622 - val_loss: 0.9241\n",
      "Epoch 10/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5908 - loss: 1.0306 - val_accuracy: 0.6622 - val_loss: 0.9084\n",
      "Epoch 11/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6183 - loss: 0.9685 - val_accuracy: 0.6554 - val_loss: 0.9040\n",
      "Epoch 12/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5761 - loss: 1.0021 - val_accuracy: 0.6554 - val_loss: 0.9025\n",
      "Epoch 13/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6083 - loss: 0.9968 - val_accuracy: 0.6554 - val_loss: 0.8957\n",
      "Epoch 14/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6179 - loss: 0.9675 - val_accuracy: 0.6486 - val_loss: 0.8937\n",
      "Epoch 15/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6073 - loss: 0.9712 - val_accuracy: 0.6486 - val_loss: 0.8890\n",
      "Epoch 16/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5381 - loss: 1.0535 - val_accuracy: 0.6419 - val_loss: 0.8867\n",
      "Epoch 17/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6036 - loss: 1.0055 - val_accuracy: 0.6486 - val_loss: 0.8766\n",
      "Epoch 18/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6429 - loss: 0.9314 - val_accuracy: 0.6419 - val_loss: 0.8746\n",
      "Epoch 19/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6082 - loss: 0.9340 - val_accuracy: 0.6486 - val_loss: 0.8721\n",
      "Epoch 20/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6292 - loss: 0.9653 - val_accuracy: 0.6554 - val_loss: 0.8648\n",
      "Epoch 21/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6250 - loss: 0.9166 - val_accuracy: 0.6351 - val_loss: 0.8713\n",
      "Epoch 22/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6144 - loss: 0.9123 - val_accuracy: 0.6486 - val_loss: 0.8628\n",
      "Epoch 23/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6150 - loss: 0.9237 - val_accuracy: 0.6622 - val_loss: 0.8673\n",
      "Epoch 24/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5977 - loss: 0.9235 - val_accuracy: 0.6757 - val_loss: 0.8503\n",
      "Epoch 25/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5872 - loss: 0.9744 - val_accuracy: 0.6351 - val_loss: 0.8608\n",
      "Epoch 26/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6491 - loss: 0.8945 - val_accuracy: 0.6486 - val_loss: 0.8543\n",
      "Epoch 27/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6240 - loss: 0.9366 - val_accuracy: 0.6824 - val_loss: 0.8596\n",
      "Epoch 28/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5798 - loss: 0.9538 - val_accuracy: 0.6689 - val_loss: 0.8548\n",
      "Epoch 29/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6315 - loss: 0.8943 - val_accuracy: 0.6351 - val_loss: 0.8748\n",
      "Epoch 30/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6550 - loss: 0.8870 - val_accuracy: 0.6554 - val_loss: 0.8564\n",
      "Epoch 31/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6330 - loss: 0.8801 - val_accuracy: 0.6689 - val_loss: 0.8570\n",
      "Epoch 32/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6257 - loss: 0.8962 - val_accuracy: 0.6622 - val_loss: 0.8608\n",
      "Epoch 33/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6231 - loss: 0.8826 - val_accuracy: 0.6554 - val_loss: 0.8566\n",
      "Epoch 34/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5945 - loss: 0.9039 - val_accuracy: 0.6689 - val_loss: 0.8480\n",
      "Epoch 35/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6325 - loss: 0.8869 - val_accuracy: 0.6689 - val_loss: 0.8477\n",
      "Epoch 36/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6033 - loss: 0.9157 - val_accuracy: 0.6554 - val_loss: 0.8599\n",
      "Epoch 37/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6427 - loss: 0.8765 - val_accuracy: 0.6622 - val_loss: 0.8524\n",
      "Epoch 38/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6327 - loss: 0.8672 - val_accuracy: 0.6622 - val_loss: 0.8497\n",
      "Epoch 39/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6356 - loss: 0.8944 - val_accuracy: 0.6689 - val_loss: 0.8612\n",
      "Epoch 40/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6516 - loss: 0.8526 - val_accuracy: 0.6757 - val_loss: 0.8650\n",
      "Epoch 41/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6176 - loss: 0.8785 - val_accuracy: 0.6622 - val_loss: 0.8590\n",
      "Epoch 42/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6447 - loss: 0.8463 - val_accuracy: 0.6554 - val_loss: 0.8668\n",
      "Epoch 43/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6820 - loss: 0.8307 - val_accuracy: 0.6757 - val_loss: 0.8596\n",
      "Epoch 44/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6445 - loss: 0.8336 - val_accuracy: 0.6757 - val_loss: 0.8446\n",
      "Epoch 45/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6693 - loss: 0.8351 - val_accuracy: 0.6622 - val_loss: 0.8604\n",
      "Epoch 46/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6798 - loss: 0.7947 - val_accuracy: 0.6689 - val_loss: 0.8716\n",
      "Epoch 47/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6645 - loss: 0.7980 - val_accuracy: 0.6554 - val_loss: 0.8802\n",
      "Epoch 48/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6373 - loss: 0.8212 - val_accuracy: 0.6351 - val_loss: 0.8756\n",
      "Epoch 49/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6324 - loss: 0.8523 - val_accuracy: 0.6419 - val_loss: 0.8823\n",
      "Epoch 50/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6801 - loss: 0.8149 - val_accuracy: 0.6351 - val_loss: 0.8815\n",
      "Epoch 51/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6466 - loss: 0.8250 - val_accuracy: 0.6149 - val_loss: 0.8921\n",
      "Epoch 52/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6865 - loss: 0.7666 - val_accuracy: 0.6486 - val_loss: 0.8808\n",
      "Epoch 53/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6345 - loss: 0.8172 - val_accuracy: 0.6486 - val_loss: 0.8704\n",
      "Epoch 54/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6609 - loss: 0.7949 - val_accuracy: 0.6554 - val_loss: 0.8594\n",
      "Epoch 55/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6204 - loss: 0.8324 - val_accuracy: 0.6419 - val_loss: 0.8595\n",
      "Epoch 56/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6647 - loss: 0.7776 - val_accuracy: 0.6419 - val_loss: 0.8675\n",
      "Epoch 57/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6711 - loss: 0.8084 - val_accuracy: 0.6284 - val_loss: 0.8865\n",
      "Epoch 58/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6657 - loss: 0.8153 - val_accuracy: 0.6486 - val_loss: 0.8836\n",
      "Epoch 59/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6508 - loss: 0.8220 - val_accuracy: 0.6486 - val_loss: 0.8908\n",
      "Epoch 60/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6255 - loss: 0.8946 - val_accuracy: 0.6554 - val_loss: 0.8909\n",
      "Epoch 61/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6657 - loss: 0.8109 - val_accuracy: 0.6486 - val_loss: 0.8988\n",
      "Epoch 62/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6837 - loss: 0.7635 - val_accuracy: 0.6351 - val_loss: 0.8925\n",
      "Epoch 63/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6933 - loss: 0.7496 - val_accuracy: 0.6351 - val_loss: 0.9054\n",
      "Epoch 64/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6745 - loss: 0.7622 - val_accuracy: 0.6419 - val_loss: 0.9050\n",
      "Epoch 65/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7170 - loss: 0.7371 - val_accuracy: 0.6419 - val_loss: 0.9090\n",
      "Epoch 66/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6953 - loss: 0.7604 - val_accuracy: 0.6554 - val_loss: 0.9092\n",
      "Epoch 67/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6821 - loss: 0.7579 - val_accuracy: 0.6554 - val_loss: 0.9062\n",
      "Epoch 68/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6628 - loss: 0.7868 - val_accuracy: 0.6419 - val_loss: 0.9077\n",
      "Epoch 69/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6925 - loss: 0.7083 - val_accuracy: 0.6351 - val_loss: 0.8990\n",
      "Epoch 70/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6684 - loss: 0.7730 - val_accuracy: 0.6216 - val_loss: 0.8932\n",
      "Epoch 71/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6749 - loss: 0.7976 - val_accuracy: 0.6486 - val_loss: 0.8866\n",
      "Epoch 72/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6876 - loss: 0.7265 - val_accuracy: 0.6419 - val_loss: 0.8985\n",
      "Epoch 73/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6523 - loss: 0.7943 - val_accuracy: 0.6351 - val_loss: 0.8955\n",
      "Epoch 74/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6811 - loss: 0.7143 - val_accuracy: 0.6486 - val_loss: 0.9007\n",
      "Epoch 75/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7275 - loss: 0.6731 - val_accuracy: 0.6284 - val_loss: 0.9177\n",
      "Epoch 76/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6784 - loss: 0.7501 - val_accuracy: 0.6351 - val_loss: 0.9046\n",
      "Epoch 77/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7091 - loss: 0.7108 - val_accuracy: 0.6554 - val_loss: 0.9103\n",
      "Epoch 78/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6994 - loss: 0.7694 - val_accuracy: 0.6689 - val_loss: 0.9134\n",
      "Epoch 79/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7208 - loss: 0.6825 - val_accuracy: 0.6554 - val_loss: 0.9238\n",
      "Epoch 80/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6877 - loss: 0.7634 - val_accuracy: 0.6622 - val_loss: 0.9126\n",
      "Epoch 81/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6963 - loss: 0.7487 - val_accuracy: 0.6554 - val_loss: 0.9138\n",
      "Epoch 82/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7061 - loss: 0.7114 - val_accuracy: 0.6216 - val_loss: 0.9184\n",
      "Epoch 83/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7227 - loss: 0.6694 - val_accuracy: 0.6554 - val_loss: 0.9139\n",
      "Epoch 84/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6937 - loss: 0.7811 - val_accuracy: 0.6486 - val_loss: 0.9124\n",
      "Epoch 85/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6787 - loss: 0.7233 - val_accuracy: 0.6419 - val_loss: 0.9254\n",
      "Epoch 86/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6894 - loss: 0.7299 - val_accuracy: 0.6216 - val_loss: 0.9132\n",
      "Epoch 87/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7053 - loss: 0.7668 - val_accuracy: 0.6419 - val_loss: 0.9163\n",
      "Epoch 88/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7167 - loss: 0.6932 - val_accuracy: 0.6486 - val_loss: 0.9181\n",
      "Epoch 89/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7224 - loss: 0.6686 - val_accuracy: 0.6351 - val_loss: 0.9377\n",
      "Epoch 90/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6721 - loss: 0.7609 - val_accuracy: 0.6419 - val_loss: 0.9207\n",
      "Epoch 91/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7467 - loss: 0.6631 - val_accuracy: 0.6486 - val_loss: 0.9116\n",
      "Epoch 92/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7333 - loss: 0.6625 - val_accuracy: 0.6554 - val_loss: 0.9261\n",
      "Epoch 93/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7173 - loss: 0.6624 - val_accuracy: 0.6216 - val_loss: 0.9245\n",
      "Epoch 94/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7210 - loss: 0.7159 - val_accuracy: 0.6216 - val_loss: 0.9509\n",
      "Epoch 95/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6684 - loss: 0.7333 - val_accuracy: 0.6284 - val_loss: 0.9305\n",
      "Epoch 96/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6684 - loss: 0.7586 - val_accuracy: 0.6149 - val_loss: 0.9159\n",
      "Epoch 97/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7248 - loss: 0.6664 - val_accuracy: 0.6351 - val_loss: 0.9432\n",
      "Epoch 98/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7086 - loss: 0.7363 - val_accuracy: 0.6081 - val_loss: 0.9374\n",
      "Epoch 99/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7073 - loss: 0.6802 - val_accuracy: 0.6351 - val_loss: 0.9198\n",
      "Epoch 100/100\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6799 - loss: 0.7110 - val_accuracy: 0.6284 - val_loss: 0.9232\n",
      "Test Accuracy: 0.54347825050354\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.93      0.81        75\n",
      "           1       0.58      0.39      0.47        54\n",
      "           2       0.22      0.24      0.23        25\n",
      "           3       0.13      0.12      0.12        26\n",
      "           4       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.54       184\n",
      "   macro avg       0.33      0.34      0.33       184\n",
      "weighted avg       0.51      0.54      0.52       184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachc\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\sachc\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\sachc\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('heart.csv')\n",
    "\n",
    "# Identify categorical columns and apply label encoding\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_cols:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# Split data into features and target variable\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "\n",
    "# One-hot encode the target variable\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Impute missing values in the feature matrix\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.3))  # Dropout for regularization\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))  # Output layer with softmax activation\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=16, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Predict and generate classification report\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"Classification Report:\\n\", classification_report(y_true_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93e83fc-6ed0-4a31-a100-ca35a5933985",
   "metadata": {},
   "source": [
    "## Heart Disease Prediction Model with Random Forest\n",
    "\n",
    "In this section, I implemented a heart disease prediction model using a **Random Forest Classifier**. Random Forest is an ensemble learning method that operates by constructing multiple decision trees during training and outputs the mode of the classes for classification.\n",
    "\n",
    "### Data Preparation and Preprocessing\n",
    "\n",
    "1. **Missing Values Handling**: \n",
    "   - Checked for missing values in the dataset and imputed them. Categorical columns were filled with their mode, while numerical columns were filled with their mean.\n",
    "\n",
    "2. **Label Encoding**: \n",
    "   - Categorical columns were converted into numeric format using `LabelEncoder` to facilitate model training.\n",
    "\n",
    "3. **Feature and Target Preparation**: \n",
    "   - The dataset was split into features (`X`) and the target variable (`y`), where the target variable indicates the presence (1) or absence (0) of heart disease.\n",
    "\n",
    "4. **Handling Class Imbalance**: \n",
    "   - Applied **SMOTE (Synthetic Minority Over-sampling Technique)** to address class imbalance in the dataset. This technique generates synthetic samples for the minority class, leading to a more balanced dataset.\n",
    "\n",
    "5. **Train-Test Split**: \n",
    "   - The resampled dataset was split into training and testing sets (80% training, 20% testing).\n",
    "\n",
    "6. **Feature Standardization**: \n",
    "   - Standardized the features using `StandardScaler` to ensure all input features are on the same scale, which can improve model performance.\n",
    "\n",
    "### Model Training\n",
    "\n",
    "- A Random Forest model was trained using 100 decision trees (`n_estimators=100`).\n",
    "\n",
    "### Results\n",
    "\n",
    "- The Random Forest model achieved an impressive **test accuracy of 87%**, significantly improving the model's performance compared to previous attempts using Logistic Regression (54%) and the Neural Network (54%).\n",
    "\n",
    "```plaintext\n",
    "Test Accuracy: 0.87\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d13f131a-1fac-4461-ac54-73805ef78ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " id            0\n",
      "age           0\n",
      "sex           0\n",
      "dataset       0\n",
      "cp            0\n",
      "trestbps     59\n",
      "chol         30\n",
      "fbs          90\n",
      "restecg       2\n",
      "thalch       55\n",
      "exang        55\n",
      "oldpeak      62\n",
      "slope       309\n",
      "ca          611\n",
      "thal        486\n",
      "num           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sachc\\AppData\\Local\\Temp\\ipykernel_270132\\881291406.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].mean(), inplace=True)\n",
      "C:\\Users\\sachc\\AppData\\Local\\Temp\\ipykernel_270132\\881291406.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].mode()[0], inplace=True)\n",
      "C:\\Users\\sachc\\AppData\\Local\\Temp\\ipykernel_270132\\881291406.py:19: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data[column].fillna(data[column].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86        85\n",
      "           1       0.90      0.67      0.77        81\n",
      "           2       0.83      0.88      0.85        72\n",
      "           3       0.85      0.89      0.87        84\n",
      "           4       0.96      0.99      0.97        89\n",
      "\n",
      "    accuracy                           0.87       411\n",
      "   macro avg       0.87      0.87      0.86       411\n",
      "weighted avg       0.87      0.87      0.87       411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('heart.csv')\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in each column:\\n\", data.isnull().sum())\n",
    "\n",
    "# Impute missing values (filling numerical columns with mean and categorical with mode)\n",
    "for column in data.columns:\n",
    "    if data[column].dtype == 'object':\n",
    "        # Fill categorical columns with mode\n",
    "        data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "    else:\n",
    "        # Fill numerical columns with mean\n",
    "        data[column].fillna(data[column].mean(), inplace=True)\n",
    "\n",
    "# Convert categorical columns to numeric using Label Encoding\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "label_encoder = LabelEncoder()\n",
    "for column in categorical_cols:\n",
    "    data[column] = label_encoder.fit_transform(data[column])\n",
    "\n",
    "# Prepare features and target\n",
    "X = data.drop(columns=['num'])\n",
    "y = data['num']\n",
    "\n",
    "# Perform SMOTE to handle class imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Split resampled data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model performance\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f490f17a-81b7-4e4a-81c4-4e597de208cc",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for Random Forest Classifier Using Grid Search\n",
    "\n",
    "In this section, I employed **Grid Search** to optimize the hyperparameters of the **Random Forest Classifier**. Hyperparameter tuning is a crucial step in improving model performance, as it helps in finding the best combination of parameters that yield the highest predictive accuracy.\n",
    "\n",
    "### Model Definition\n",
    "\n",
    "The initial Random Forest model was defined with a fixed random state for reproducibility:\n",
    "\n",
    "```python\n",
    "rf_model = RandomForestClassifier(random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0e306-2105-4ebd-9dfc-ec7b7e0118c1",
   "metadata": {},
   "source": [
    "### Parameter Grid Definition\n",
    "To explore different hyperparameter combinations, I defined a parameter grid that included:\n",
    "\n",
    "n_estimators: Number of trees in the forest. Tested values: 100, 200.\n",
    "max_depth: Maximum depth of the tree. Tested values: None, 10, 20, 30.\n",
    "min_samples_split: Minimum number of samples required to split an internal node. Tested values: 2, 5, 10.\n",
    "class_weight: Weights associated with classes. Tested values: 'balanced', None.\n",
    "\n",
    "### Grid Search Implementation\n",
    "A GridSearchCV object was created with the Random Forest model and the defined parameter grid. The grid search utilized 5-fold cross-validation and was optimized for the weighted F1 score:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44320e0-dbcc-4d2c-9e49-d2c7d7251129",
   "metadata": {},
   "source": [
    "```python\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                           scoring='f1_weighted', cv=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3bdcf7-69f9-4803-b957-754ca8870b54",
   "metadata": {},
   "source": [
    "The grid search was then fitted to the training data:\n",
    "\n",
    "```python\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933feec3-4093-47d8-a331-8316966a4cbb",
   "metadata": {},
   "source": [
    "### Best Parameters\n",
    "After completing the grid search, the best hyperparameters were identified:\n",
    "\n",
    "Best parameters: \n",
    "\n",
    "```python\n",
    "Best parameters: {<best_params>}\r\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc0f4b2-65e8-4885-992b-e0cd84eedaa4",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "The best model from the grid search was evaluated on the test set, and the results were summarized in the classification report:\n",
    "\n",
    "```python\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "print(\"Best Random Forest Classification Report:\\n\", classification_report(y_test, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c94a961-9c58-4e40-9afc-b1206ac78c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best parameters: {'class_weight': 'balanced', 'max_depth': None, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Best Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.87      0.84        85\n",
      "           1       0.87      0.65      0.75        81\n",
      "           2       0.81      0.89      0.85        72\n",
      "           3       0.86      0.87      0.86        84\n",
      "           4       0.94      0.99      0.96        89\n",
      "\n",
      "    accuracy                           0.86       411\n",
      "   macro avg       0.86      0.85      0.85       411\n",
      "weighted avg       0.86      0.86      0.85       411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, \n",
    "                           scoring='f1_weighted', cv=5, verbose=1)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "print(\"Best Random Forest Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4cddaa3-3e93-4912-826b-6fcdfe696566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Downloading alembic-1.13.3-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\sachc\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sachc\\anaconda3\\lib\\site-packages (from optuna) (23.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in c:\\users\\sachc\\anaconda3\\lib\\site-packages (from optuna) (2.0.30)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sachc\\anaconda3\\lib\\site-packages (from optuna) (4.66.4)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\sachc\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\sachc\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\sachc\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.3.0->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sachc\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\sachc\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-4.0.0-py3-none-any.whl (362 kB)\n",
      "   ---------------------------------------- 0.0/362.8 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 112.6/362.8 kB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 362.8/362.8 kB 5.7 MB/s eta 0:00:00\n",
      "Downloading alembic-1.13.3-py3-none-any.whl (233 kB)\n",
      "   ---------------------------------------- 0.0/233.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 233.2/233.2 kB 14.9 MB/s eta 0:00:00\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Downloading Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.6/78.6 kB 4.3 MB/s eta 0:00:00\n",
      "Installing collected packages: Mako, colorlog, alembic, optuna\n",
      "Successfully installed Mako-1.3.6 alembic-1.13.3 colorlog-6.9.0 optuna-4.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5569425-72d0-4755-bd4c-cefdd7b3efad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optuna Intergrations\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 5, 50)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    class_weight = trial.suggest_categorical(\"class_weight\", [\"balanced\", None])\n",
    "\n",
    "    # Create the model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        class_weight=class_weight,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Return the F1 score as the objective metric to maximize\n",
    "    return f1_score(y_test, y_pred, average='weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "907e87d9-f5f3-4c45-bb63-3cb2ddde29b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-29 22:12:56,791] A new study created in memory with name: no-name-118ec5ca-db92-440a-8e25-7375976f3a45\n",
      "[I 2024-10-29 22:12:57,472] Trial 0 finished with value: 0.6504695776620535 and parameters: {'n_estimators': 173, 'max_depth': 42, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:12:58,391] Trial 1 finished with value: 0.6451905658068154 and parameters: {'n_estimators': 241, 'max_depth': 35, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:12:59,261] Trial 2 finished with value: 0.5732056888770635 and parameters: {'n_estimators': 208, 'max_depth': 46, 'min_samples_split': 4, 'class_weight': None}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:12:59,701] Trial 3 finished with value: 0.54549127343245 and parameters: {'n_estimators': 114, 'max_depth': 37, 'min_samples_split': 8, 'class_weight': None}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:00,206] Trial 4 finished with value: 0.5510262105194128 and parameters: {'n_estimators': 134, 'max_depth': 36, 'min_samples_split': 8, 'class_weight': None}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:00,754] Trial 5 finished with value: 0.57310794217782 and parameters: {'n_estimators': 137, 'max_depth': 48, 'min_samples_split': 3, 'class_weight': None}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:00,981] Trial 6 finished with value: 0.5698343287769586 and parameters: {'n_estimators': 51, 'max_depth': 13, 'min_samples_split': 3, 'class_weight': None}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:01,661] Trial 7 finished with value: 0.6494943912939618 and parameters: {'n_estimators': 181, 'max_depth': 50, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:02,325] Trial 8 finished with value: 0.5518882128673315 and parameters: {'n_estimators': 171, 'max_depth': 14, 'min_samples_split': 8, 'class_weight': None}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:02,561] Trial 9 finished with value: 0.6173897503735593 and parameters: {'n_estimators': 53, 'max_depth': 25, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:03,663] Trial 10 finished with value: 0.6309438646425503 and parameters: {'n_estimators': 288, 'max_depth': 23, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:04,405] Trial 11 finished with value: 0.6406162363156692 and parameters: {'n_estimators': 195, 'max_depth': 50, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:05,411] Trial 12 finished with value: 0.6419178323031173 and parameters: {'n_estimators': 250, 'max_depth': 41, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:06,131] Trial 13 finished with value: 0.6267676102699145 and parameters: {'n_estimators': 170, 'max_depth': 42, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:06,541] Trial 14 finished with value: 0.6047115558842836 and parameters: {'n_estimators': 99, 'max_depth': 5, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:07,871] Trial 15 finished with value: 0.629448667349724 and parameters: {'n_estimators': 214, 'max_depth': 32, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:08,542] Trial 16 finished with value: 0.6347494991659751 and parameters: {'n_estimators': 154, 'max_depth': 43, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:09,511] Trial 17 finished with value: 0.6351706409873966 and parameters: {'n_estimators': 239, 'max_depth': 29, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 0 with value: 0.6504695776620535.\n",
      "[I 2024-10-29 22:13:09,911] Trial 18 finished with value: 0.6511993770308737 and parameters: {'n_estimators': 92, 'max_depth': 50, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:10,294] Trial 19 finished with value: 0.5604982861105927 and parameters: {'n_estimators': 92, 'max_depth': 44, 'min_samples_split': 2, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:10,671] Trial 20 finished with value: 0.6304936038808913 and parameters: {'n_estimators': 88, 'max_depth': 39, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:11,487] Trial 21 finished with value: 0.6441661730501426 and parameters: {'n_estimators': 190, 'max_depth': 50, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:12,104] Trial 22 finished with value: 0.6507286076042857 and parameters: {'n_estimators': 147, 'max_depth': 46, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:12,651] Trial 23 finished with value: 0.6360942321846123 and parameters: {'n_estimators': 123, 'max_depth': 45, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:13,007] Trial 24 finished with value: 0.6371734105323925 and parameters: {'n_estimators': 73, 'max_depth': 32, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:13,641] Trial 25 finished with value: 0.6465073774856382 and parameters: {'n_estimators': 148, 'max_depth': 46, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:14,179] Trial 26 finished with value: 0.6301436306917509 and parameters: {'n_estimators': 115, 'max_depth': 40, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:14,861] Trial 27 finished with value: 0.6364696371579162 and parameters: {'n_estimators': 160, 'max_depth': 47, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:15,201] Trial 28 finished with value: 0.6100341691777873 and parameters: {'n_estimators': 71, 'max_depth': 40, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:16,104] Trial 29 finished with value: 0.6455053819464398 and parameters: {'n_estimators': 229, 'max_depth': 34, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:16,676] Trial 30 finished with value: 0.6419178323031173 and parameters: {'n_estimators': 136, 'max_depth': 20, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:17,352] Trial 31 finished with value: 0.6499651607205498 and parameters: {'n_estimators': 182, 'max_depth': 50, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:18,202] Trial 32 finished with value: 0.6446369424767305 and parameters: {'n_estimators': 202, 'max_depth': 47, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:19,109] Trial 33 finished with value: 0.6447731190520968 and parameters: {'n_estimators': 218, 'max_depth': 44, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:19,731] Trial 34 finished with value: 0.5491562406248959 and parameters: {'n_estimators': 184, 'max_depth': 38, 'min_samples_split': 9, 'class_weight': None}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:20,609] Trial 35 finished with value: 0.5627553146227902 and parameters: {'n_estimators': 258, 'max_depth': 48, 'min_samples_split': 8, 'class_weight': None}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:21,065] Trial 36 finished with value: 0.6356898726749984 and parameters: {'n_estimators': 145, 'max_depth': 50, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:21,425] Trial 37 finished with value: 0.5883175176687737 and parameters: {'n_estimators': 124, 'max_depth': 45, 'min_samples_split': 7, 'class_weight': None}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:21,861] Trial 38 finished with value: 0.6424080309640666 and parameters: {'n_estimators': 103, 'max_depth': 35, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:22,605] Trial 39 finished with value: 0.5664783790380624 and parameters: {'n_estimators': 169, 'max_depth': 48, 'min_samples_split': 9, 'class_weight': None}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:23,344] Trial 40 finished with value: 0.6393902956195344 and parameters: {'n_estimators': 162, 'max_depth': 37, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:24,141] Trial 41 finished with value: 0.6494943912939618 and parameters: {'n_estimators': 181, 'max_depth': 50, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:25,071] Trial 42 finished with value: 0.6446369424767305 and parameters: {'n_estimators': 203, 'max_depth': 48, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:25,881] Trial 43 finished with value: 0.6375029653260866 and parameters: {'n_estimators': 174, 'max_depth': 43, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:26,412] Trial 44 finished with value: 0.6356898726749984 and parameters: {'n_estimators': 145, 'max_depth': 46, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:27,191] Trial 45 finished with value: 0.6438415650372172 and parameters: {'n_estimators': 192, 'max_depth': 42, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:28,521] Trial 46 finished with value: 0.6104507916798955 and parameters: {'n_estimators': 300, 'max_depth': 50, 'min_samples_split': 4, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:29,231] Trial 47 finished with value: 0.5569826084539896 and parameters: {'n_estimators': 160, 'max_depth': 46, 'min_samples_split': 8, 'class_weight': None}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:30,083] Trial 48 finished with value: 0.629448667349724 and parameters: {'n_estimators': 214, 'max_depth': 18, 'min_samples_split': 6, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:30,791] Trial 49 finished with value: 0.6173441859526252 and parameters: {'n_estimators': 178, 'max_depth': 6, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:31,411] Trial 50 finished with value: 0.6356212482608166 and parameters: {'n_estimators': 130, 'max_depth': 48, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:32,161] Trial 51 finished with value: 0.6499651607205498 and parameters: {'n_estimators': 182, 'max_depth': 49, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:32,871] Trial 52 finished with value: 0.6443590140018711 and parameters: {'n_estimators': 196, 'max_depth': 49, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:33,822] Trial 53 finished with value: 0.6446369424767305 and parameters: {'n_estimators': 226, 'max_depth': 44, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:34,711] Trial 54 finished with value: 0.6406162363156692 and parameters: {'n_estimators': 188, 'max_depth': 41, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:35,491] Trial 55 finished with value: 0.6507286076042857 and parameters: {'n_estimators': 169, 'max_depth': 47, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:36,115] Trial 56 finished with value: 0.6354057841647884 and parameters: {'n_estimators': 151, 'max_depth': 27, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:36,842] Trial 57 finished with value: 0.6507286076042857 and parameters: {'n_estimators': 167, 'max_depth': 46, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:37,293] Trial 58 finished with value: 0.645663287341334 and parameters: {'n_estimators': 111, 'max_depth': 42, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:37,954] Trial 59 finished with value: 0.614766056924442 and parameters: {'n_estimators': 166, 'max_depth': 45, 'min_samples_split': 7, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:38,275] Trial 60 finished with value: 0.6326890641217853 and parameters: {'n_estimators': 66, 'max_depth': 47, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:39,031] Trial 61 finished with value: 0.6494943912939618 and parameters: {'n_estimators': 157, 'max_depth': 49, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:39,702] Trial 62 finished with value: 0.6494943912939618 and parameters: {'n_estimators': 175, 'max_depth': 46, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:40,331] Trial 63 finished with value: 0.6364391123329589 and parameters: {'n_estimators': 143, 'max_depth': 43, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:41,319] Trial 64 finished with value: 0.5978894355891474 and parameters: {'n_estimators': 201, 'max_depth': 49, 'min_samples_split': 2, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:42,124] Trial 65 finished with value: 0.6406162363156692 and parameters: {'n_estimators': 184, 'max_depth': 47, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:42,909] Trial 66 finished with value: 0.6507286076042857 and parameters: {'n_estimators': 169, 'max_depth': 44, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:43,318] Trial 67 finished with value: 0.5642659176783312 and parameters: {'n_estimators': 85, 'max_depth': 39, 'min_samples_split': 9, 'class_weight': None}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:43,961] Trial 68 finished with value: 0.6359029723444511 and parameters: {'n_estimators': 135, 'max_depth': 41, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:44,715] Trial 69 finished with value: 0.6351592370575088 and parameters: {'n_estimators': 167, 'max_depth': 44, 'min_samples_split': 5, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:45,297] Trial 70 finished with value: 0.6353444157042357 and parameters: {'n_estimators': 152, 'max_depth': 45, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:45,931] Trial 71 finished with value: 0.6504695776620535 and parameters: {'n_estimators': 174, 'max_depth': 48, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:46,661] Trial 72 finished with value: 0.6507286076042857 and parameters: {'n_estimators': 172, 'max_depth': 47, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:47,421] Trial 73 finished with value: 0.6507286076042857 and parameters: {'n_estimators': 169, 'max_depth': 43, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:48,194] Trial 74 finished with value: 0.6364696371579162 and parameters: {'n_estimators': 164, 'max_depth': 40, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:48,821] Trial 75 finished with value: 0.6510756764501241 and parameters: {'n_estimators': 125, 'max_depth': 42, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 18 with value: 0.6511993770308737.\n",
      "[I 2024-10-29 22:13:49,441] Trial 76 finished with value: 0.6514970732883731 and parameters: {'n_estimators': 127, 'max_depth': 43, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:50,025] Trial 77 finished with value: 0.640744158199943 and parameters: {'n_estimators': 119, 'max_depth': 37, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:50,551] Trial 78 finished with value: 0.6511993770308737 and parameters: {'n_estimators': 110, 'max_depth': 46, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:51,081] Trial 79 finished with value: 0.6509870266267075 and parameters: {'n_estimators': 104, 'max_depth': 45, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:51,566] Trial 80 finished with value: 0.5393773379080755 and parameters: {'n_estimators': 100, 'max_depth': 32, 'min_samples_split': 10, 'class_weight': None}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:52,100] Trial 81 finished with value: 0.6511993770308737 and parameters: {'n_estimators': 107, 'max_depth': 45, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:52,581] Trial 82 finished with value: 0.6511993770308737 and parameters: {'n_estimators': 108, 'max_depth': 45, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:53,045] Trial 83 finished with value: 0.6418556941381294 and parameters: {'n_estimators': 109, 'max_depth': 45, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:53,441] Trial 84 finished with value: 0.6509870266267075 and parameters: {'n_estimators': 90, 'max_depth': 42, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:53,892] Trial 85 finished with value: 0.6425365320888623 and parameters: {'n_estimators': 93, 'max_depth': 39, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:54,261] Trial 86 finished with value: 0.6449976743640634 and parameters: {'n_estimators': 80, 'max_depth': 41, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:54,838] Trial 87 finished with value: 0.64298733620816 and parameters: {'n_estimators': 124, 'max_depth': 42, 'min_samples_split': 10, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:55,331] Trial 88 finished with value: 0.6509870266267075 and parameters: {'n_estimators': 105, 'max_depth': 43, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:55,791] Trial 89 finished with value: 0.6511993770308737 and parameters: {'n_estimators': 94, 'max_depth': 43, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:56,172] Trial 90 finished with value: 0.6511993770308737 and parameters: {'n_estimators': 95, 'max_depth': 38, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:56,597] Trial 91 finished with value: 0.6511993770308737 and parameters: {'n_estimators': 94, 'max_depth': 38, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:57,031] Trial 92 finished with value: 0.6511993770308737 and parameters: {'n_estimators': 96, 'max_depth': 36, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:57,491] Trial 93 finished with value: 0.6105005507005634 and parameters: {'n_estimators': 95, 'max_depth': 34, 'min_samples_split': 3, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:57,918] Trial 94 finished with value: 0.6262340155471714 and parameters: {'n_estimators': 81, 'max_depth': 36, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:58,205] Trial 95 finished with value: 0.6332298383395532 and parameters: {'n_estimators': 65, 'max_depth': 38, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:58,742] Trial 96 finished with value: 0.645952918918133 and parameters: {'n_estimators': 116, 'max_depth': 40, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:59,081] Trial 97 finished with value: 0.5350526942411148 and parameters: {'n_estimators': 77, 'max_depth': 38, 'min_samples_split': 10, 'class_weight': None}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:59,484] Trial 98 finished with value: 0.6424080309640666 and parameters: {'n_estimators': 98, 'max_depth': 35, 'min_samples_split': 8, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n",
      "[I 2024-10-29 22:13:59,902] Trial 99 finished with value: 0.6400136164188805 and parameters: {'n_estimators': 84, 'max_depth': 33, 'min_samples_split': 9, 'class_weight': 'balanced'}. Best is trial 76 with value: 0.6514970732883731.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  F1 Score: 0.6514970732883731\n",
      "  Params:\n",
      "    n_estimators: 127\n",
      "    max_depth: 43\n",
      "    min_samples_split: 9\n",
      "    class_weight: balanced\n"
     ]
    }
   ],
   "source": [
    "# Create a study object\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Optimize the objective function\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best trial\n",
    "best_trial = study.best_trial\n",
    "print(\"Best trial:\")\n",
    "print(f\"  F1 Score: {best_trial.value}\")\n",
    "print(\"  Params:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "859b4dad-baa1-49f8-b4cd-4ff3e882ef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.88      0.84        85\n",
      "           1       0.87      0.59      0.71        81\n",
      "           2       0.75      0.88      0.81        72\n",
      "           3       0.85      0.86      0.85        84\n",
      "           4       0.91      0.97      0.94        89\n",
      "\n",
      "    accuracy                           0.84       411\n",
      "   macro avg       0.84      0.83      0.83       411\n",
      "weighted avg       0.84      0.84      0.83       411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the best model with the best hyperparameters\n",
    "best_params = best_trial.params\n",
    "best_model = RandomForestClassifier(\n",
    "    n_estimators=best_params[\"n_estimators\"],\n",
    "    max_depth=best_params[\"max_depth\"],\n",
    "    min_samples_split=best_params[\"min_samples_split\"],\n",
    "    class_weight=best_params[\"class_weight\"],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the model on the entire training data\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred_final = best_model.predict(X_test)\n",
    "print(\"Final Classification Report:\\n\", classification_report(y_test, y_pred_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700ed2a-e737-456b-966b-1f242317ea58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338ec17-daed-4e9c-a052-acc8542b35eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
